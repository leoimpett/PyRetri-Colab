{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyRetri.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skcb4PI1HcTG",
        "outputId": "4e035849-94b8-4ce9-e60f-eefcdcf06d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/leoimpett/PyRetri-Colab &>/dev/null\n",
        "%cd /content/PyRetri-Colab/\n",
        "!pip install -r requirements.txt  &>/dev/null\n",
        "!python  setup.py install &>/dev/null\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import tqdm\n",
        "import multiprocessing\n",
        "\n",
        "!mkdir ycba_images\n",
        "!mkdir ycba_images/imgs\n",
        "\n",
        "with open(\"/content/PyRetri-Colab/ycbaprints.json\", \"r\") as myfile:\n",
        "    data = json.load(myfile)\n",
        "    \n",
        "\n",
        "imkeys = []\n",
        "imURLs = {}\n",
        "imtitles = {}\n",
        "\n",
        "for item in data[\"results\"][\"result\"]:\n",
        "    thiskey = item[\"binding\"][2][\"uri\"]\n",
        "    if thiskey not in imkeys:\n",
        "        imkeys.append(thiskey)\n",
        "        imURLs[thiskey] = item[\"binding\"][1][\"uri\"]\n",
        "        imtitles[thiskey] = item[\"binding\"][0][\"literal\"]\n",
        "        \n",
        "def makeFilePath(imkey):\n",
        "    endkey = imkey.split(\"/\")[-1]\n",
        "    return \"ycba_images/imgs/\" + endkey + \".jpg\"\n",
        "\n",
        "\n",
        "def downloadImage(thiskey):\n",
        "    url = imURLs[thiskey]\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open( makeFilePath(thiskey) , 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    return 0\n",
        "\n",
        "\n",
        "pool = multiprocessing.Pool(16)\n",
        "\n",
        "_ = pool.map(downloadImage, imkeys)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyRetri-Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKW1uYiKv_bX"
      },
      "source": [
        "# Extract features from the Gallery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh_z_fdMqL0N",
        "outputId": "b432a155-c4ce-4e16-9ff3-d4114eac882d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "images_data_path = \"/content/PyRetri-Colab/ycbaprints/\"\n",
        "test_im = '/content/PyRetri-Colab/ycbaprints/imgs/2003.jpg'\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "import tqdm\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "from pyretri.config import get_defaults_cfg, setup_cfg\n",
        "from pyretri.datasets import build_folder, build_loader\n",
        "from pyretri.models import build_model\n",
        "from pyretri.extract import build_extract_helper\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "from pyretri.extract import make_data_json\n",
        "\n",
        "make_data_json(images_data_path,\"./data_jsons/gdrive.json\",\"general\")\n",
        "\n",
        "# init and load retrieval pipeline settings\n",
        "cfg = get_defaults_cfg()\n",
        "cfg = setup_cfg(cfg, \"configs/gdrive.yaml\", [])\n",
        "\n",
        "# build dataset and dataloader\n",
        "dataset = build_folder(\"./data_jsons/gdrive.json\", cfg.datasets)\n",
        "dataloader = build_loader(dataset, cfg.datasets)\n",
        "\n",
        "# build model\n",
        "model = build_model(cfg.model)\n",
        "\n",
        "# build helper and extract features\n",
        "extract_helper = build_extract_helper(model, cfg.extract)\n",
        "extract_helper.do_extract(dataloader, \"./data/features/gdrive\", save_interval=5000)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a3d0b105867b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyretri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_data_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmake_data_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./data_jsons/gdrive.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"general\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# init and load retrieval pipeline settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/PyRetri-Colab/pyretri/extract/utils/make_data_json.py\u001b[0m in \u001b[0;36mmake_data_json\u001b[0;34m(dataset_path, save_path, type, gt_path)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'general'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oxford'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'general'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mmake_ds_for_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'oxford'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mmake_ds_for_oxford\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/PyRetri-Colab/pyretri/extract/utils/make_data_json.py\u001b[0m in \u001b[0;36mmake_ds_for_general\u001b[0;34m(dataset_path, save_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0minfo_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimg_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlabel_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/1625/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYeIElNbwEPe"
      },
      "source": [
        "# Now extract them from a test image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNStvGeFdavi"
      },
      "source": [
        "!mkdir retrieved_images\n",
        "!rm retrieved_images/*\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from pyretri.config import get_defaults_cfg, setup_cfg\n",
        "from pyretri.datasets import build_transformers\n",
        "from pyretri.models import build_model\n",
        "from pyretri.extract import build_extract_helper\n",
        "from pyretri.index import build_index_helper, feature_loader\n",
        "\n",
        "\n",
        "\n",
        "config_file = \"configs/gdrive.yaml\"\n",
        "\n",
        "\n",
        "# init and load retrieval pipeline settings\n",
        "cfg = get_defaults_cfg()\n",
        "cfg = setup_cfg(cfg, config_file, [])\n",
        "\n",
        "# set path for single image\n",
        "path = test_im\n",
        "\n",
        "# build transformers\n",
        "transformers = build_transformers(cfg.datasets.transformers)\n",
        "\n",
        "# build model\n",
        "model = build_model(cfg.model)\n",
        "\n",
        "# read image and convert it to tensor\n",
        "img = Image.open(path).convert(\"RGB\")\n",
        "img_tensor = transformers(img)\n",
        "\n",
        "# build helper and extract feature for single image\n",
        "extract_helper = build_extract_helper(model, cfg.extract)\n",
        "img_fea_info = extract_helper.do_single_extract(img_tensor)\n",
        "stacked_feature = list()\n",
        "for name in cfg.index.feature_names:\n",
        "    assert name in img_fea_info[0], \"invalid feature name: {} not in {}!\".format(name, img_fea_info[0].keys())\n",
        "    stacked_feature.append(img_fea_info[0][name].cpu())\n",
        "img_fea = np.concatenate(stacked_feature, axis=1)\n",
        "\n",
        "# load gallery features\n",
        "gallery_fea, gallery_info, _ = feature_loader.load(cfg.index.gallery_fea_dir, cfg.index.feature_names)\n",
        "\n",
        "# build helper and single index feature\n",
        "index_helper = build_index_helper(cfg.index)\n",
        "\n",
        "\n",
        "#index_result_info, query_fea, gallery_fea = index_helper.do_index(img_fea, img_fea_info, gallery_fea)\n",
        "\n",
        "#index_helper.save_topk_retrieved_images('retrieved_images/', index_result_info[0], 5, gallery_info)\n",
        "\n",
        "#print('single index have done!')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoR7aU2N0n_c"
      },
      "source": [
        "index_result_info, query_fea, gallery_fea = index_helper.do_index(gallery_fea, gallery_info, gallery_fea)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggRMcPcV1vr7"
      },
      "source": [
        "print(index_result_info[0][\"ranked_neighbors_idx\"])\n",
        "print(len(index_result_info[0][\"ranked_neighbors_idx\"]))\n",
        "print(len(index_result_info))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8bKaFbu4Rtg"
      },
      "source": [
        "import json\n",
        "\n",
        "!rm jsonfiles/*\n",
        "!mkdir jsonfiles\n",
        "\n",
        "for element in gallery_info:\n",
        "  elidx = element[\"idx\"]\n",
        "  filep = element[\"path\"]\n",
        "  filen = filep.split(\"/\")[-1].split(\".\")[0]\n",
        "  nearest = element[\"ranked_neighbors_idx\"]\n",
        "  \n",
        "  nearData = []\n",
        "  for jdx in nearest:\n",
        "    nearData.append({\"filename\":gallery_info[jdx][\"path\"].split(\"/\")[-1].split(\".\")[0],\"similarity\":1.0,\"caption\":\"Test\"})\n",
        "\n",
        "  jsonfn = \"jsonfiles/\" + filen + \".json\"\n",
        "  with open(jsonfn, \"w\") as jfile:\n",
        "    json.dump(nearData,jfile)\n",
        "\n",
        "\n",
        "presentData = []\n",
        "for element in gallery_info:\n",
        "  presentData.append({\"image\":element[\"path\"].split(\"/\")[-1],\"caption\":\"Test\"})\n",
        "\n",
        "print(len(presentData))\n",
        "with open(\"randomImagePaths.json\", \"w\") as jfile:\n",
        "  json.dump(presentData,jfile)\n",
        "\n",
        "\n",
        "!zip -r jsonfiles.zip jsonfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZDyv6Wb0DsR"
      },
      "source": [
        "plt.imshow(io.imread(test_im))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjvRjnrRddDs"
      },
      "source": [
        "imlist = glob.glob(\"./retrieved_images/*\")\n",
        "for iml in imlist:\n",
        "  plt.figure()\n",
        "  plt.imshow(io.imread(iml))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9u_uV7Gg3k2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}